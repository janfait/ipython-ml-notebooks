{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML101 : Neural Network\n",
    "14-09-2016,  \n",
    "Jan Fait,  \n",
    "Digital Marketing,  \n",
    "Munich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Network, The Hype\n",
    "\n",
    "Most people imagine something like this and back away in fear.  \n",
    "This is indeed what it looks like, but without the 3D graphics.\n",
    "\n",
    "![](https://takinginitiative.files.wordpress.com/2008/04/neural-network-rotator.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rewind to perceptron\n",
    "\n",
    "You have seen the perceptron in the first of your classes.\n",
    "The perceptron - yes, you can call it a **neuron** - is linear separator.\n",
    "\n",
    "It can separate linearly separable data - it can draw a line between a bunch of points.\n",
    "\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_07.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLP\n",
    "\n",
    "Very often, you see the term MLP Network.\n",
    "\n",
    "This is essentially a **M**ulti-**L**ayer-**P**erceptron.\n",
    "\n",
    "Input layer are the perceptrons fed with a input vector, Hidden layer are some more perceptrons.\n",
    "\n",
    "\n",
    "![](http://www.texample.net/media/tikz/examples/PNG/neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the extra layer for? The exclusive or (XOR) problem.\n",
    "\n",
    "Remember. Perceptron only does linearly separable problems.\n",
    "\n",
    "Take boolean inputs **a** and **b**.  \n",
    "We want the output to be **True** if: \n",
    "\n",
    "(**a** and **not b**) or (**not a** and **b**)\n",
    "\n",
    "![](http://srv-marketing1.muc.ecircle.de/utils/xor.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logical_xor(a, b):\n",
    "    return bool(a) ^ bool(b)\n",
    "\n",
    "logical_xor(1,0), logical_xor(0,1), logical_xor(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Something as simple as XOR is not linearly separable.\n",
    "\n",
    "We need a **combined** decision of the nodes. One decision on the **OR**, one decision on the **AND**.\n",
    "\n",
    "![](http://uk-calling.com/other/uni/xor.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the numbers on the connections?\n",
    "\n",
    "Remember perceptron, these are **the weights**. Whatever comes from the input is multiplied by a connection weight before reaching the next layer.\n",
    "\n",
    "\n",
    "**A=0 and B=1**: \n",
    "+ sum(A,B) > 1/2 but also sum(A,B) < 3/2 \n",
    "+ upper node is activated resulting in a 1 \n",
    "+ lower node is not activated resulting in a 0. \n",
    "+ (1 x 1) + (0 x -1) > 1/2 so the output is 1.\n",
    "\n",
    "**A=1 and B=1**:\n",
    "\n",
    "+ sum(A,B) > 1/2 and sum(A,B) > 3/2\n",
    "+ both nodes get activated\n",
    "+ (1 x 1) + (1 x -1) < 1/2 so the output is 0.\n",
    "\n",
    "![](http://uk-calling.com/other/uni/xor.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How did we get to those 1/2 and 3/2 numbers though?\n",
    "\n",
    " We didn't, the network did. Through training. They are the results of several runs of the A,B pairs through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation for training the network\n",
    "\n",
    "Backpropagation is the most common way the network learns the right weights.\n",
    "\n",
    "Steps it takes:\n",
    "\n",
    "+ Initialize random weights W1,W2 on the layers\n",
    "\n",
    "+ Feed the input matrix X into the first layer and on to others\n",
    "\n",
    "$Layer0 = X$  \n",
    "$Layer1 = nonlinear(W1 * Layer0)$  \n",
    "$Layer2 = nonlinear(W2 * Layer1)$  \n",
    "\n",
    "+ Compare the result of this with the expected Y vector\n",
    "\n",
    "+ Caluculate error in prediction\n",
    "\n",
    "+ Feed this error back through the network \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does the nonlinear function thing do?\n",
    "\n",
    "+ It is an activation function.   \n",
    "+ It defines, how much should the output react to the input of the previous layer.  \n",
    "+ A multilayer network using only linear activation functions is equivalent to some single layer, linear network.  \n",
    "+ In other words, in order to make a linear transformation, you don't need an extra layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAhnuklQgw12SCvT/kWgTSYRFEYEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7dc56a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "\n",
    "scaling_input = [-20,-10,-5,-2,-1,-0.5,-0.1,0,0.1,0.5,1,2,5,10,20];\n",
    "nonlinear_scaling = [];\n",
    "\n",
    "for i in scaling_input :\n",
    "    nonlinear_scaling.append(nonlin(i))   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(scaling_input,nonlinear_scaling, 'ro')\n",
    "plt.axis([-20,20, -0.1, 1.1])\n",
    "plt.show()   \n",
    "#this is our non-linear transformation function\n",
    "#it recieves an input and scales between 0 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#input matrix\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "\n",
    "#expected output matrix\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "#set random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "# 3x4 matrix for weights of each of the layers (input,layer1,layer2 x 4 nodes)\n",
    "weights0 = 2*np.random.random((3,4)) - 1\n",
    "# 1x4 matrix for the output\n",
    "weights1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors:\n",
      "[[-0.47372957]\n",
      " [ 0.51104304]\n",
      " [ 0.45615914]\n",
      " [-0.54470837]]\n",
      "Mean Absolute Error:0.496410031903\n",
      "Adjusting weights by deltas:\n",
      "[[-0.29194348]\n",
      " [ 0.31677549]\n",
      " [ 0.28861437]\n",
      " [-0.34474975]]\n",
      "Errors:\n",
      "[[-0.00258945]\n",
      " [ 0.00182417]\n",
      " [ 0.00141689]\n",
      " [-0.00259128]]\n",
      "Mean Absolute Error:0.00210544735288\n",
      "Adjusting weights by deltas:\n",
      "[[-0.0012964 ]\n",
      " [ 0.00133292]\n",
      " [ 0.00103543]\n",
      " [-0.00129732]]\n",
      "Errors:\n",
      "[[-0.00137415]\n",
      " [ 0.00093858]\n",
      " [ 0.0007877 ]\n",
      " [-0.00137466]]\n",
      "Mean Absolute Error:0.00111877251414\n",
      "Adjusting weights by deltas:\n",
      "[[-0.00068754]\n",
      " [ 0.00068599]\n",
      " [ 0.00057574]\n",
      " [-0.0006878 ]]\n",
      "Errors:\n",
      "[[-0.00093787]\n",
      " [ 0.00063115]\n",
      " [ 0.00054915]\n",
      " [-0.00093812]]\n",
      "Mean Absolute Error:0.000764071627177\n",
      "Adjusting weights by deltas:\n",
      "[[-0.00046916]\n",
      " [ 0.00046133]\n",
      " [ 0.0004014 ]\n",
      " [-0.00046928]]\n",
      "Errors:\n",
      "[[-0.00071252]\n",
      " [ 0.00047518]\n",
      " [ 0.0004225 ]\n",
      " [-0.00071267]]\n",
      "Mean Absolute Error:0.000580718687431\n",
      "Adjusting weights by deltas:\n",
      "[[-0.00035639]\n",
      " [ 0.00034734]\n",
      " [ 0.00030884]\n",
      " [-0.00035646]]\n",
      "Errors:\n",
      "[[-0.00057473]\n",
      " [ 0.00038093]\n",
      " [ 0.00034372]\n",
      " [-0.00057483]]\n",
      "Mean Absolute Error:0.000468553920876\n",
      "Adjusting weights by deltas:\n",
      "[[-0.00028745]\n",
      " [ 0.00027846]\n",
      " [ 0.00025126]\n",
      " [-0.0002875 ]]\n"
     ]
    }
   ],
   "source": [
    "for j in range(60000):\n",
    "\n",
    "    # feed forward through layers 0, 1, and 2\n",
    "    layer0 = X\n",
    "    #dot product of the input matrix and the first layer of weights\n",
    "    layer1 = nonlin(np.dot(layer0,weights0))\n",
    "    #dot product of the output of the layer 1 with weights of layer 2\n",
    "    layer2 = nonlin(np.dot(layer1,weights1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    layer2_error = y - layer2\n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Errors:\")\n",
    "        print(layer2_error)\n",
    "    \n",
    "    #on every 10000 interations print mean error of classification\n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Mean Absolute Error:\" + str(np.mean(np.abs(layer2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    layer2_delta = layer2_error*nonlin(layer2,deriv=False)\n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Adjusting weights by deltas:\")\n",
    "        print(layer2_delta)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    layer1_error = layer2_delta.dot(weights1.T)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    # this is where scaling is needed - if we didn't scale, we would adjust too much.\n",
    "    layer1_delta = layer1_error * nonlin(layer1,deriv=False)\n",
    "    \n",
    "    #adjust the weights on the layer by their product with the deltas\n",
    "    # = multiply output delta and layer input activation to get the gradient of the weight.\n",
    "    weights1 += layer1.T.dot(layer2_delta)\n",
    "    weights0 += layer0.T.dot(layer1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How many hidden layers?\n",
    "\n",
    "As always. Consider the complexity of the problem. \n",
    "\n",
    "**Too few** - the network will not be able to grasp the dimensionality of the problem and ends up being too simple.\n",
    "\n",
    "**Too many** - the network overfits - learns the training data, not the underlying problem\n",
    "\n",
    "![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/nn-from-scratch-hidden-layer-varying.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "We understood, how the neural network works:\n",
    "\n",
    "+ It is essentially a number of perceptrons in multiple layers\n",
    "+ The input is fed into the first layer, multiplied by its node's weights, scaled and sent further\n",
    "+ Scaling allows solution of non-linear problems and makes activation of next layer nodes easier\n",
    "+ After the output layer gets to a conclusion, it is compared with the expected result and if not same, it is adjusted by a certain margin and fed back through the network where weights are adjusted\n",
    "+ We rarely see how did the network arrive to a conclusion, it is a black-box method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's it for today. Thank you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[Scikit-Learn NN module](http://scikit-learn.org/dev/modules/neural_networks_supervised.html)  \n",
    "[Excellent chapter from Nature of Code on NN](http://natureofcode.com/book/chapter-10-neural-networks/)  \n",
    "[Backpropagation step by step](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
