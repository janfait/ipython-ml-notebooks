{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML101 : Text Classification, Spam Detection\n",
    "07-09-2016,  \n",
    "Jan Fait,  \n",
    "Digital Marketing,  \n",
    "Munich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition\n",
    "\n",
    "\n",
    "### How is text classification similar to usual supervised classification tasks?\n",
    "+ There are labels/classes the model should learn\n",
    "+ The input is a collection of character strings -> a document\n",
    "+ A combination of input strings predicts a label\n",
    "\n",
    "> To turn a document into a vector of predictors we need a **document representation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document representation\n",
    "\n",
    "Did not find a nice secular example\n",
    "\n",
    "![](http://www.python-course.eu/images/document_representation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The problem with many documents \n",
    "\n",
    "Imagine the outcome of several hundred documents represented as shown before.\n",
    "The results turn out:\n",
    "\n",
    "+ **Wide** - there are many many words = too many predictors\n",
    "+ **Sparse** - only a few words are found in all documents\n",
    "+ **Confounded** - the words found in many documents don't have any predictive ability (Hello, Kind Regards, ...)\n",
    "\n",
    "Example of a wide, sparse matrix\n",
    "![](http://i.stack.imgur.com/7H4Kj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "= selecting just the strings with predictive ability\n",
    "\n",
    "> Feature extraction is tricky, usually demands experimenting and competing models.\n",
    "\n",
    "\n",
    "**1. Compute new features**\n",
    "    + too much UPPERCASE\n",
    "    + too much !!!!!\n",
    "    + sender, IP\n",
    "**2. Standardize strings**\n",
    "    + tokenize\n",
    "    + lowercase\n",
    "    + remove punctuation\n",
    "    + probabilistic spelling correction\n",
    "    + lemmatize\n",
    "**3. Remove stopwords**\n",
    "    + you, me, a, the\n",
    "**4. Set a % of documents the feature should be in**\n",
    "    + only words which are in 5% + documents\n",
    "    + longer than 3 characters\n",
    "\n",
    "Lemmatization (see [Lucene](https://github.com/larsmans/lucene-stanford-lemmatizer) for implementations):\n",
    "![Lemmatization](https://www.briggsby.com/wp-content/uploads/2014/11/lemmatization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional probability\n",
    "\n",
    "*Two following slides adapted from Alexan.org [](https://alexn.org/)*\n",
    "\n",
    "Data:\n",
    "\n",
    "    30 emails out of a total of 74 are spam messages = P(spam)\n",
    "    51 emails out of those 74 contain the word “penis” = P(penis)\n",
    "    20 emails containing the word “penis” have been marked as spam = P(spam|penis)\n",
    " \n",
    "We know what is the $probability(penis | spam)$, it is $\\frac{20}{74}$\n",
    "But what we really wanna know is the $probability(spam|penis)$\n",
    "\n",
    "We use a rule of conditional probability:\n",
    "\n",
    "![](https://alexn.org/assets/img/conditional-prob.png)\n",
    "\n",
    "Now, $probability(spam|penis)$ = spam GIVEN penis\n",
    "\n",
    "![](https://alexn.org/assets/img/spam-simple-bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive bayes\n",
    "\n",
    "But our data has more words than 'penis' that can be spammy, we need to consider them jointly.\n",
    "\n",
    "    25 emails out of the total also contain the word “viagra”\n",
    "    24 emails out of those have been marked as spam\n",
    "    1 remaining email is not spam\n",
    "\n",
    "> To avoid doing Conditional probability on every single word, we assume their independence!\n",
    "\n",
    "So we get $probability(spam|penis,viagra)$ by:\n",
    "\n",
    "![](https://alexn.org/assets/img/spam-multiple-bayes-naive.png)\n",
    "\n",
    "Conversely, $probability(ham|penis,viagra)$ is $\\frac$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical \n",
    "\n",
    "We take a nearly identical appraoch in loading data as at the last talk.\n",
    "\n",
    "+ Getting data into \n",
    "+ Using Scikit-learn library for modelling\n",
    "\n",
    "\n",
    "##### Data : SMS Spam Collection\n",
    "\n",
    "5574 short messages dataset. Get it [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text class\n",
      "0  Go until jurong point, crazy.. Available only ...   ham\n",
      "1                      Ok lar... Joking wif u oni...   ham\n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
      "3  U dun say so early hor... U c already then say...   ham\n",
      "4  Nah I don't think he goes to usf, he lives aro...   ham\n",
      "5  FreeMsg Hey there darling it's been 3 week's n...  spam\n",
      "6  Even my brother is not like to speak with me. ...   ham\n",
      "7  As per your request 'Melle Melle (Oru Minnamin...   ham\n",
      "8  WINNER!! As a valued network customer you have...  spam\n",
      "9  Had your mobile 11 months or more? U R entitle...  spam\n",
      "Number of training rows is: 4459\n"
     ]
    }
   ],
   "source": [
    "#pandas is a popular package for working with tabular data as data.frames, yes almost like Spark data.frames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#read in the data, define separator\n",
    "df = pd.read_csv('../data/datasets/spamcollection.csv', sep=';')\n",
    "#show a truncated data frame\n",
    "print(df[:10])\n",
    "\n",
    "#simple validation set taking 0.8 of data, defined random seed\n",
    "train=df.sample(frac=0.8,random_state=200)\n",
    "#inverse selection for the test set\n",
    "test=df.drop(train.index)\n",
    "#check\n",
    "print(\"Number of training rows is: \"+ str(len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Standardization\n",
    "\n",
    "The below code shows examples of how to standardize text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2.0,\n",
       " 'buy': 1.0,\n",
       " 'ever': 1.0,\n",
       " 'now': 1.0,\n",
       " 'online': 1.0,\n",
       " 'russian': 2.0,\n",
       " 'wanted': 1.0,\n",
       " 'wife': 2.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#define translations for a elements in string.punctuation object\n",
    "def remove_punctuation(s):\n",
    "    table = s.maketrans({key: None for key in string.punctuation})\n",
    "    return s.translate(table)\n",
    "\n",
    "#ingest text and     \n",
    "def tokenize(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = text.lower()\n",
    "    return re.split(\"\\W+\", text)\n",
    "\n",
    "def count_words(words):\n",
    "    wc = {}\n",
    "    for word in words:\n",
    "        wc[word] = wc.get(word, 0.0) + 1.0\n",
    "    return wc\n",
    "\n",
    "s = \"Ever wanted a russian wife? Buy a russian wife online now.\"\n",
    "count_words(tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Enter the CountVectorizer class\n",
    "\n",
    "[Scikit docs for count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "CountVectorizer does almost the same thing as the above \n",
    "+ Learns the total vocabulary\n",
    "+ Gets counts of words for documents\n",
    "+ Cleans up\n",
    "+ Does indexation for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 20)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 15)\t1\n"
     ]
    }
   ],
   "source": [
    "#load the scikit library\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#count vectorizer it learns the vocabulary of the corpus and extracts word count features. \n",
    "count_vectorizer1 = CountVectorizer(max_features=1000, lowercase=True)\n",
    "count_vectorizer2 = CountVectorizer(max_features=100, lowercase=True)\n",
    "text_sample = df['text'].values\n",
    "#just taking the first two messages\n",
    "text_sample = df['text'].head(2)\n",
    "counts_sample = count_vectorizer1.fit_transform(text_sample)\n",
    "print(counts_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam' 'ham' 'spam' ..., 'ham' 'ham' 'ham']\n",
      "['spam' 'ham' 'ham' ..., 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#vectorize the training set\n",
    "counts_train1 = count_vectorizer1.fit_transform(train['text'].values)\n",
    "counts_train2 = count_vectorizer2.fit_transform(train['text'].values)\n",
    "targets_train = train['class'].values\n",
    "#this is the actual classifier\n",
    "classifier1 = MultinomialNB()\n",
    "classifier2 = MultinomialNB()\n",
    "classifier1.fit(counts_train1, targets_train)\n",
    "classifier2.fit(counts_train2, targets_train)\n",
    "\n",
    "#vectorize the training set\n",
    "counts_test1 = count_vectorizer1.fit_transform(test['text'].values)\n",
    "counts_test2 = count_vectorizer2.fit_transform(test['text'].values)\n",
    "targets_test = test['class'].values\n",
    "\n",
    "#copy the array to form the expected\n",
    "expected = targets_test\n",
    "\n",
    "#run the test counts through the classifier\n",
    "predicted_m1 = classifier1.predict(counts_test1)\n",
    "predicted_m2 = classifier2.predict(counts_test2)\n",
    "#look at predictions\n",
    "print(predicted_m1)\n",
    "print(predicted_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Accuracy\n",
    "\n",
    "Accuracy is the number of correct predictions (guessed the right class) in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76860986547085197, 0.85919282511210759)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hint: we prefer to use vectorized functions\n",
    "def getAccuracy(exp,pre):\n",
    "    return sum(exp==pre) /len(exp==pre)\n",
    "\n",
    "\n",
    "\n",
    "getAccuracy(expected,predicted_m1),getAccuracy(expected,predicted_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Confusion Matrix\n",
    "\n",
    "The confusion matrix shows all possible results of the (expected == predicted) comparison\n",
    "![](http://www.gepsoft.com/gepsoft/APS3KB/Chapter09/Section2/confusionmatrix.png)\n",
    "\n",
    "Let us break it apart.\n",
    "\n",
    "> TP = **True Positive** = Expected True == Predicted True  \n",
    "> FN = **False Negative** = Expected True != Predicted False  \n",
    "> FP = **False Positive** = Expected False != Predicted True  \n",
    "> TN = **True Negative** = Expected False == Predicted False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Accuracy: $A = \\frac{(TP + TN)}{TP+FN+FP+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[888,  88],\n",
       "       [ 69,  70]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "#confusion matrix 1\n",
    "confusion_matrix(expected,predicted_m1)\n",
    "#confusion matrix 2\n",
    "confusion_matrix(expected,predicted_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beyond accuracy\n",
    "\n",
    "$Sensitivity/Recall = \\frac{TP}{TP+FN}$ - Catching the True Positives True==True correctly  \n",
    "$Specificity = \\frac{TN}{TN+FP}$ - Catching the True Negatives False==False correctly  \n",
    "\n",
    "If your model has a very high value of one of the above, but totally fails on the other one, its wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificiy1 0.727969348659 specificiy1 0.556962025316\n",
      "sensitivity1 0.805327868852 sensitivity2 0.909836065574\n"
     ]
    }
   ],
   "source": [
    "def getSens(exp,pre):\n",
    "    cm = confusion_matrix(exp, pre)\n",
    "    return cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "\n",
    "#but we have to define our own for specificity\n",
    "def getSpec(exp,pre):\n",
    "    cm = confusion_matrix(exp, pre)\n",
    "    return cm[0,1]/(cm[0,1]+cm[1,1])\n",
    "\n",
    "spec1 = getSpec(expected, predicted_m1)\n",
    "spec2 = getSpec(expected, predicted_m2)\n",
    "\n",
    "sens1 = getSens(expected, predicted_m1)\n",
    "sens2 = getSens(expected, predicted_m2)\n",
    "print(\"specificiy1\",spec1,\"specificiy1\",spec2)\n",
    "print(\"sensitivity1\",sens1,\"sensitivity2\",sens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "We saw how to classify text with Naive Bayes, the important take-aways are:\n",
    "\n",
    "+ standardizing text input\n",
    "+ basic feature selection\n",
    "+ workings of the simplified Bayes rule in the Naive Bayes\n",
    "+ although assuming independence conditional probabilities of two words and their effect on the outcome is courageous, the algorithm works.\n",
    "+ remember accuracy is one thing, but always look at the confusion matrix before optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's it for today. Thank you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[Scikit-Learn Metrics module](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)  \n",
    "[Naive Bayes in 5 minutes](https://www.youtube.com/watch?v=IlVINQDk4o8)  \n",
    "[Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
