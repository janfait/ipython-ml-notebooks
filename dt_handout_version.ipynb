{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML101 : Inside the decision tree classifier\n",
    "10-08-2016  \n",
    "Jan Fait, jan.fait@teradata.com  \n",
    "Digital Marketing  \n",
    "Munich  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Getting ready\n",
    "\n",
    "+ You may get your Python IDE ready if interested\n",
    "+ This tutorial draws heavily from Programing Collective Intelligence, Chapter 7, Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Decision trees are a supervised (machine)-learning technique\n",
    "\n",
    "+ Supervised learning models classify new data based on previously available data.  \n",
    "+ They know what combination of inputs led to what outcomes. \n",
    "+ Tree classifies a new data point based on a series of conditional statements. \n",
    "\n",
    "------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### Examples of what decision trees commonly solve\n",
    "\n",
    "* Fraud detection  \n",
    "* Customer/Item classification based on usage, properites\n",
    "* Conversion prediction\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "### Decision trees are good because\n",
    "\n",
    "\n",
    " + Easily understood, you see how stuff happens\n",
    " + No parametric demands and assumptions about distributions\n",
    " + Automated feature selection and variable importance\n",
    " + Little pre-processing of data needed, easy missing value treatment\n",
    " + Both categorical and continuous predictors\n",
    "\n",
    "**Quick to deploy, good start, everybody knows what's goin on**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "### Decision trees are bad because\n",
    "\n",
    " - Very eager to overfit\n",
    " - Correlation between inputs confuses the split algorithm\n",
    " \n",
    " \n",
    "**Check for too specific, arbitrary split rules that likely don't apply outside of the input sample**\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### A beer tree\n",
    " \n",
    "+ Input = [brand,price]\n",
    "+ Output = [purchase decision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "beer_data=[['paulaner',2,'no'],\n",
    "           ['paulaner',3,'no'],\n",
    "           ['augustiner',3,'no'],\n",
    "           ['augustiner',3,'yes'],\n",
    "           ['paulaner',0,'no'], #oh, look at this, so true\n",
    "           ['augustiner',3,'yes'],\n",
    "           ['augustiner',3,'yes'],\n",
    "           ['augustiner',3,'no'],\n",
    "           ['paulaner',2,'no'],\n",
    "           ['paulaner',3,'no']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Paulaner-sucks tree](/files/tree.jpg)\n",
    "\n",
    "\n",
    "Easy, but how **exactly** did the tree do it? \n",
    "\n",
    ">How does it know to put the brand as the first node?\n",
    "\n",
    ">What are we trying to achieve on the end nodes?\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### The CART algorithm\n",
    "(classification-and-regression-trees algorithm, Breiman, 1984)  \n",
    "See Introduction for Statistical Learning, p.303 for regression trees\n",
    "\n",
    "\n",
    "> The algorithm relies on recursive-partioning of the dataset by maximizing the **purity** of the outcome columns in the resulting sets.\n",
    "\n",
    "Core question CART asks itself : \n",
    "> Does this split lead to a more pure node with respect to the outcome column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The CART algorithm\n",
    "highest_purity=purity(data)  \n",
    "purity_improvement=0  \n",
    "best_split = []  \n",
    "\n",
    "For each column in data\n",
    "....For each value in unique values of column\n",
    "\n",
    ".........A = data[column==value]   \n",
    ".........B = data[column!=value]  \n",
    ".........Compute purity[value,column] = len(A)/len(data) purity(A) + len(B)/len(data) purity(B).  \n",
    ".........If purity[value,column] < highest_purity  \n",
    "............purity_improvement = highest_purity - purity[value,column]  \n",
    "............highest_purity = purity[value,column]  \n",
    "............best_split = [A,B]  \n",
    "return best_split  \n",
    "...If purity_improvement > 0  \n",
    "......For each subset of best_split  \n",
    ".........For each column in subset ... and so on\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### How to measure purity of a split\n",
    "\n",
    "Every time the CART algorithm proposes a split on a value, we check how pure the 2 resulting sets are with respect to the outcome variable.\n",
    "\n",
    "The most common methods (for classification) are:\n",
    "\n",
    "###### Entropy\n",
    "\n",
    "The amount of disorder in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6500224216483541, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_mixed_set = [\"cat\",\"cat\",\"cat\",\"cat\",\"cat\",\"dog\"]\n",
    "more_mixed_set = [\"cat\",\"cat\",\"cat\",\"dog\",\"dog\",\"dog\"]\n",
    "\n",
    "# @x = list of values \n",
    "# get unique values of the list, for each of them check how many there are as a share of list length (p)\n",
    "# multiply the p by its log(), add up the products.\n",
    "# (as the p is (0-1) logs are 0< by definition, to add up negatives, we do -= decrement)\n",
    "\n",
    "def entropy(x):\n",
    "   from math import log\n",
    "   log2=lambda y:log(y)/log(2)\n",
    "   uniquevals=list(set(x))\n",
    "   ent = 0.0\n",
    "   for val in uniquevals:\n",
    "    p = len([item for item in x if item==val])/len(x)\n",
    "    ent -= p*log2(p)\n",
    "   return ent\n",
    "\n",
    "entropy(less_mixed_set), entropy(more_mixed_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Gini Impurity\n",
    "It is the theorerical probability of misclassifying when taking a random item from a set and assigning a random label to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2777777777777778, 0.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @x = list of values\n",
    "#get unique values of the list, for each of them check how many there are as a share of list length (p1)\n",
    "#multiply this p1 by a probabilities of other items in the list\n",
    "#add up probabilities\n",
    "\n",
    "def gini(x):\n",
    "   uniquevals=list(set(x))\n",
    "   g = 0.0\n",
    "   for val1 in uniquevals:\n",
    "    p1 = len([item for item in x if item==val1])/len(x)\n",
    "    for val2 in uniquevals:\n",
    "        if val1==val2: continue\n",
    "        p2 = len([item for item in x if item==val2])/len(x)\n",
    "        g += p1*p2\n",
    "   return g\n",
    "\n",
    "gini(less_mixed_set),gini(more_mixed_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "###### Information gain\n",
    "\n",
    "For the next steps, we chose **entropy** as it is stricter. Look at the less_mixed_set, just 1 dog among 5 cats brings entropy to 0.65. Which is actually quite accurate to real-life.\n",
    "\n",
    "Entropy is our purity measure, but **Information gain** is actually what decides which split to take first. See below how to get it. It is the difference between current node and potential split node entropies. The split that has the highest information gain is taken.\n",
    "\n",
    "![Information gain with entropy](/files/information_gain.jpg)\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 2. Structure of our program\n",
    "\n",
    "\n",
    "### What do we need\n",
    "\n",
    "1. function that does the looping through input columns and their values and divides data into sets\n",
    "2. function that computes the purity of the divided sets\n",
    "3. function that tells us if the purity is pure enough to do the split\n",
    "4. way to create a recursive application of the above\n",
    "5. function that displays the tree\n",
    "\n",
    "### Program in pseudo-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    " function build_decision_tree (data){\n",
    "        \n",
    "        old_entropy = entropy(data)\n",
    "        best_information_gain = 0\n",
    "        minimal_gain_needed = 0\n",
    "        best_splits = null\n",
    "        \n",
    "            for each column of data:\n",
    "                for each value of column\n",
    "                    splits[] = **divide(data,value,column)** (1.)\n",
    "                    new_entropy = **entropy(splits)** (2.)\n",
    "                    new_information_gain = old_entropy - new_entropy\n",
    "                    \n",
    "                    if **new_information_gain > best_information_gain** (3.)\n",
    "                        best_information_gain = new_information_gain\n",
    "                        best_splits = splits\n",
    "        \n",
    "            if **best_information_gain > minimal_gain_needed**\n",
    "                **branch0 = build_decision_tree(splits[0])** (4.)\n",
    "                branch1 = build_decision_tree(splits[1])\n",
    "                return split_node \n",
    "            else\n",
    "                return end_node\n",
    "             \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### Program in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Divide function\n",
    "def divideset(rows,column,value):\n",
    "   split_function=None\n",
    "   if isinstance(value,int) or isinstance(value,float):\n",
    "      split_function=lambda row:row[column]>=value\n",
    "   else:\n",
    "      split_function=lambda row:row[column]==value\n",
    "   set1=[row for row in rows if split_function(row)]\n",
    "   set2=[row for row in rows if not split_function(row)]\n",
    "   return (set1,set2)\n",
    "\n",
    "# Helper function to create counts of possible results (the last column of each row is the result)\n",
    "def uniquecounts(rows):\n",
    "   results={}\n",
    "   for row in rows:\n",
    "      # The result is the last column\n",
    "      r=row[len(row)-1]\n",
    "      if r not in results: results[r]=0\n",
    "      results[r]+=1\n",
    "   return results\n",
    "\n",
    "#  2. Edited entropy to be applied in this scenario\n",
    "def entropy(rows):\n",
    "   from math import log\n",
    "   log2=lambda x:log(x)/log(2)  \n",
    "   results=uniquecounts(rows)\n",
    "   # Now calculate the entropy\n",
    "   ent=0.0\n",
    "   for r in results.keys():\n",
    "      p=float(results[r])/len(rows)\n",
    "      ent=ent-p*log2(p)\n",
    "   return ent\n",
    "\n",
    "# Helper class to build the decision node that is meant to be returned\n",
    "class decisionnode:\n",
    "  def __init__(self,col=-1,value=None,results=None,tb=None,fb=None):\n",
    "    self.col=col\n",
    "    self.value=value\n",
    "    self.results=results\n",
    "    self.tb=tb\n",
    "    self.fb=fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now on to the actual program as outlined in pseudo-code above. Notice we pass the entropy function as a purity measure. This setup allows us to pass gini or other if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def buildtree(rows,scoref=entropy):\n",
    "  if len(rows)==0: return decisionnode()\n",
    "  current_score=scoref(rows)\n",
    "\n",
    "  best_gain=0.0\n",
    "  best_criteria=None\n",
    "  best_sets=None\n",
    "  \n",
    "  column_count=len(rows[0])-1\n",
    "  for col in range(0,column_count):\n",
    "    column_values={}\n",
    "    for row in rows:\n",
    "       column_values[row[col]]=1\n",
    "    for value in column_values.keys():\n",
    "      (set1,set2)=divideset(rows,col,value)\n",
    "      \n",
    "      # Information gain\n",
    "      p=float(len(set1))/len(rows)\n",
    "      gain=current_score-p*scoref(set1)-(1-p)*scoref(set2)\n",
    "      if gain>best_gain and len(set1)>0 and len(set2)>0:\n",
    "        best_gain=gain\n",
    "        best_criteria=(col,value)\n",
    "        best_sets=(set1,set2)\n",
    "  # Create the sub branches   \n",
    "  if best_gain>0:\n",
    "    trueBranch=buildtree(best_sets[0])\n",
    "    falseBranch=buildtree(best_sets[1])\n",
    "    return decisionnode(col=best_criteria[0],value=best_criteria[1],\n",
    "                        tb=trueBranch,fb=falseBranch)\n",
    "  else:\n",
    "      return decisionnode(results=uniquecounts(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### Running the program on data\n",
    "\n",
    "Take beer data and extend them with another variable of whether the beer is light (helles) or dark (dunkles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "beer_data2=[['paulaner',2,'light','yes'],\n",
    "           ['paulaner',3,'dark','no'],\n",
    "           ['augustiner',5,'dark','no'],\n",
    "           ['augustiner',5,'light','no'],\n",
    "           ['augustiner',5,'light','no'],\n",
    "           ['paulaner',2,'light','yes'],\n",
    "           ['augustiner',3,'light','yes'],\n",
    "           ['augustiner',3,'light','yes'],\n",
    "           ['augustiner',5,'dark','no'],\n",
    "           ['paulaner',2,'light','yes'],\n",
    "           ['paulaner',3,'light','no'],\n",
    "           ['paulaner',3,'light','no'],\n",
    "           ['paulaner',3,'dark','no'],\n",
    "           ['paulaner',3,'dark','no'],\n",
    "           ['augustiner',4,'light','no'],\n",
    "           ['paulaner',2,'dark','yes'],\n",
    "           ['paulaner',2,'dark','yes'],\n",
    "           ['augustiner',4,'light','no'],\n",
    "           ['augustiner',4,'dark','yes'],\n",
    "           ['augustiner',4,'dark','yes'],\n",
    "           ['augustiner',4,'dark','yes'],\n",
    "           ['augustiner',4,'dark','yes'],\n",
    "           ['augustiner',4,'dark','yes'],\n",
    "           ['augustiner',4,'light','no'],\n",
    "           ['tegnerseer',7,'dark','yes'],#look at these outliers\n",
    "           ['tegnerseer',0,'dark','no']]#look at these outliers\n",
    "\n",
    "\n",
    "beer_tree = buildtree(beer_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### Display function\n",
    "\n",
    "Just for completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def getwidth(tree):\n",
    "  if tree.tb==None and tree.fb==None: return 1\n",
    "  return getwidth(tree.tb)+getwidth(tree.fb)\n",
    "\n",
    "def getdepth(tree):\n",
    "  if tree.tb==None and tree.fb==None: return 0\n",
    "  return max(getdepth(tree.tb),getdepth(tree.fb))+1\n",
    "\n",
    "\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "\n",
    "def drawtree(tree,jpeg='tree.jpg'):\n",
    "  w=getwidth(tree)*100\n",
    "  h=getdepth(tree)*100+120\n",
    "\n",
    "  img=Image.new('RGB',(w,h),(255,255,255))\n",
    "  draw=ImageDraw.Draw(img)\n",
    "\n",
    "  drawnode(draw,tree,w/2,20)\n",
    "  img.save(jpeg,'JPEG')\n",
    "  \n",
    "def drawnode(draw,tree,x,y):\n",
    "  if tree.results==None:\n",
    "    # Get the width of each branch\n",
    "    w1=getwidth(tree.fb)*100\n",
    "    w2=getwidth(tree.tb)*100\n",
    "\n",
    "    # Determine the total space required by this node\n",
    "    left=x-(w1+w2)/2\n",
    "    right=x+(w1+w2)/2\n",
    "\n",
    "    # Draw the condition string\n",
    "    draw.text((x-20,y-10),str(tree.col)+':'+str(tree.value),(0,0,0))\n",
    "\n",
    "    # Draw links to the branches\n",
    "    draw.line((x,y,left+w1/2,y+100),fill=(255,0,0))\n",
    "    draw.line((x,y,right-w2/2,y+100),fill=(255,0,0))\n",
    "    \n",
    "    # Draw the branch nodes\n",
    "    drawnode(draw,tree.fb,left+w1/2,y+100)\n",
    "    drawnode(draw,tree.tb,right-w2/2,y+100)\n",
    "  else:\n",
    "    txt=' \\n'.join(['%s:%d'%v for v in tree.results.items()])\n",
    "    draw.text((x-20,y),txt,(0,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "### Looking at the tree\n",
    "\n",
    "Our algorithm produces the below tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "drawtree(beer_tree,\"beer_tree.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Information gain with entropy](/files/beer_tree.jpg)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "## 3. Pruning tree and classifying new data\n",
    "\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Well, our tree **overfits**. See that single \"tegnerseer\" in its own branch?. It messes up the tree. Once we collect more tegnerseers, we will surely place them somewhere else in the tree. Overfitting means following the data too closely.\n",
    "\n",
    "Making predictions more generally applicable means reducing the tree's complexity --> **Pruning the tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def prune(tree,mingain):\n",
    "  # If the branches aren't leaves, then prune them\n",
    "  if tree.tb.results==None:\n",
    "    prune(tree.tb,mingain)\n",
    "  if tree.fb.results==None:\n",
    "    prune(tree.fb,mingain)\n",
    "    \n",
    "  # If both the subbranches are now leaves, see if they\n",
    "  # should merged\n",
    "  if tree.tb.results!=None and tree.fb.results!=None:\n",
    "    # Build a combined dataset\n",
    "    tb,fb=[],[]\n",
    "    for v,c in tree.tb.results.items():\n",
    "      tb+=[[v]]*c\n",
    "    for v,c in tree.fb.results.items():\n",
    "      fb+=[[v]]*c\n",
    "    \n",
    "    # Test the reduction in entropy\n",
    "    delta=entropy(tb+fb)-(entropy(tb)+entropy(fb)/2)\n",
    "\n",
    "    if delta<mingain:\n",
    "      # Merge the branches\n",
    "      tree.tb,tree.fb=None,None\n",
    "      tree.results=uniquecounts(tb+fb)\n",
    "  \n",
    "#apply pruning with a minimal information gain as a parameter\n",
    "prune(beer_tree,0.95)\n",
    "drawtree(beer_tree,\"beer_tree_pruned.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "###  Pruned tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Pruned tree](/files/beer_tree_pruned.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Why are we not checking minimal gain when building the tree?\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifying new data\n",
    "\n",
    "The point is to predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': 4, 'yes': 1}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function tells us the outcome and the node where the observation lands\n",
    "def classify(observation,tree):\n",
    "  if tree.results!=None:\n",
    "    return tree.results\n",
    "  else:\n",
    "    v=observation[tree.col]\n",
    "    branch=None\n",
    "    if isinstance(v,int) or isinstance(v,float):\n",
    "      if v>=tree.value: branch=tree.tb\n",
    "      else: branch=tree.fb\n",
    "    else:\n",
    "      if v==tree.value: branch=tree.tb\n",
    "      else: branch=tree.fb\n",
    "    return classify(observation,branch)\n",
    "\n",
    "\n",
    "\n",
    "classify(['augustiner',6,'dark'],beer_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': 5}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['paulaner',3,'light'],beer_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yes': 5}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['paulaner',2,'light'],beer_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': 3}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['augustiner',4,'light'],beer_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "## 4. What have we learned\n",
    "\n",
    " + Decision trees use information gain to decide where to split\n",
    " + Decision trees are easy to run and code without external libraries\n",
    " + Don't make the tree too specific to the sample data you are training the model on\n",
    " + Pruning helps you improve your predictive performance on new data\n",
    " --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "## 5. Coming up next\n",
    "\n",
    "One of the following topics is available:\n",
    "\n",
    "* Evaluation of Classifiers ---> Not just running, also successfuly running\n",
    "* Random Forests ---> An ensamble method for Decision Trees, taking care of overfitting\n",
    "* Matrix Factorization --> Simple but powerful numerical algortithm for dimension reduction \n",
    "* Support Vector Machines ---> Separating non-linear data with a linear method using a dimensionality blow-up\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 6. Further reading and code\n",
    "\n",
    "[Python Scikit-Learn for trees](http://scikit-learn.org/stable/modules/tree.html)   \n",
    "[Spark MLib docs for trees](https://spark.apache.org/docs/1.1.1/mllib-decision-tree.html)    \n",
    "[Overfitting on Wikipedia](https://en.wikipedia.org/wiki/Overfitting)   \n",
    "[More on overfitting](http://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/24%20-%20Decision%20Trees%203.pdf)  \n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
